{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zow/anaconda3/envs/genedrail/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/zow/anaconda3/envs/genedrail/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/home/zow/anaconda3/envs/genedrail/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/zow/anaconda3/envs/genedrail/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline, \\\n",
    "    EulerDiscreteScheduler\n",
    "import torchvision.transforms as torch_transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import os\n",
    "import os.path as osp\n",
    "from diffusion.utils import  get_formatstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_IDS = {\n",
    "    '1-1': \"CompVis/stable-diffusion-v1-1\",\n",
    "    '1-2': \"CompVis/stable-diffusion-v1-2\",\n",
    "    '1-3': \"CompVis/stable-diffusion-v1-3\",\n",
    "    '1-4': \"CompVis/stable-diffusion-v1-4\",\n",
    "    '1-5': \"runwayml/stable-diffusion-v1-5\",\n",
    "    '2-0': \"stabilityai/stable-diffusion-2-base\",\n",
    "    '2-1': \"stabilityai/stable-diffusion-2-1-base\"\n",
    "}\n",
    "\n",
    "def get_sd_model(version = '1-1' , dtype='float32'):\n",
    "    if dtype == 'float32':\n",
    "        dtype = torch.float32\n",
    "    elif dtype == 'float16':\n",
    "        dtype = torch.float16\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    assert version in MODEL_IDS.keys()\n",
    "    model_id = MODEL_IDS[version]\n",
    "    scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=dtype)\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    vae = pipe.vae\n",
    "    tokenizer = pipe.tokenizer\n",
    "    text_encoder = pipe.text_encoder\n",
    "    unet = pipe.unet\n",
    "    return vae, tokenizer, text_encoder, unet, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler_config(version= '1-1'):\n",
    "    #if version in {'1-1', '1-2', '1-3', '1-4', '1-5'}:\n",
    "    config = {\n",
    "        \"_class_name\": \"EulerDiscreteScheduler\",\n",
    "        \"_diffusers_version\": \"0.14.0\",\n",
    "        \"beta_end\": 0.012,\n",
    "        \"beta_schedule\": \"scaled_linear\",\n",
    "        \"beta_start\": 0.00085,\n",
    "        \"interpolation_type\": \"linear\",\n",
    "        \"num_train_timesteps\": 1000,\n",
    "        \"prediction_type\": \"epsilon\",\n",
    "        \"set_alpha_to_one\": False,\n",
    "        \"skip_prk_steps\": True,\n",
    "        \"steps_offset\": 1,\n",
    "        \"trained_betas\": None\n",
    "    }\n",
    "    # elif version in {'2-0', '2-1'}:\n",
    "    #     config = {\n",
    "    #         \"_class_name\": \"EulerDiscreteScheduler\",\n",
    "    #         \"_diffusers_version\": \"0.10.2\",\n",
    "    #         \"beta_end\": 0.012,\n",
    "    #         \"beta_schedule\": \"scaled_linear\",\n",
    "    #         \"beta_start\": 0.00085,\n",
    "    #         \"clip_sample\": False,\n",
    "    #         \"num_train_timesteps\": 1000,\n",
    "    #         \"prediction_type\": \"epsilon\",\n",
    "    #         \"set_alpha_to_one\": False,\n",
    "    #         \"skip_prk_steps\": True,\n",
    "    #         \"steps_offset\": 1,  # todo\n",
    "    #         \"trained_betas\": None\n",
    "    #     }\n",
    "    # else:\n",
    "    #     raise NotImplementedError\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot Classification with Stable Diffusion\n",
    "python eval_prob_adaptive.py --dataset cifar10 --split test --n_trials 1 \\\n",
    "  --to_keep 5 1 --n_samples 50 500 --loss l1 \\\n",
    "  --prompt_path prompts/cifar10_prompts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def eval_error(unet, scheduler, latent, all_noise, ts, noise_idxs,\n",
    "               text_embeds, text_embed_idxs, batch_size=32, dtype='float32', loss='l2'):\n",
    "    assert len(ts) == len(noise_idxs) == len(text_embed_idxs)\n",
    "    pred_errors = torch.zeros(len(ts), device='cpu')\n",
    "    idx = 0\n",
    "    with torch.inference_mode():\n",
    "        for _ in tqdm.trange(len(ts) // batch_size + int(len(ts) % batch_size != 0), leave=False):\n",
    "            batch_ts = torch.tensor(ts[idx: idx + batch_size])\n",
    "            noise = all_noise[noise_idxs[idx: idx + batch_size]]\n",
    "            noised_latent = latent * (scheduler.alphas_cumprod[batch_ts] ** 0.5).view(-1, 1, 1, 1).to(device) + \\\n",
    "                            noise * ((1 - scheduler.alphas_cumprod[batch_ts]) ** 0.5).view(-1, 1, 1, 1).to(device)\n",
    "            t_input = batch_ts.to(device).half() if dtype == 'float16' else batch_ts.to(device)\n",
    "            text_input = text_embeds[text_embed_idxs[idx: idx + batch_size]]\n",
    "            noise_pred = unet(noised_latent, t_input, encoder_hidden_states=text_input).sample\n",
    "            if loss == 'l2':\n",
    "                error = F.mse_loss(noise, noise_pred, reduction='none').mean(dim=(1, 2, 3))\n",
    "            elif loss == 'l1':\n",
    "                error = F.l1_loss(noise, noise_pred, reduction='none').mean(dim=(1, 2, 3))\n",
    "            elif loss == 'huber':\n",
    "                error = F.huber_loss(noise, noise_pred, reduction='none').mean(dim=(1, 2, 3))\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            pred_errors[idx: idx + len(batch_ts)] = error.detach().cpu()\n",
    "            idx += len(batch_ts)\n",
    "    return pred_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def eval_prob_adaptive(unet, latent, text_embeds, scheduler, n_samples, to_keep, batch_size, dtype, loss, latent_size=64, all_noise=None, n_trials=1):\n",
    "    scheduler_config = get_scheduler_config(scheduler)\n",
    "    T = scheduler_config['num_train_timesteps']\n",
    "    max_n_samples = max(n_samples)\n",
    "\n",
    "    if all_noise is None:\n",
    "        all_noise = torch.randn((max_n_samples * n_trials, 4, latent_size, latent_size), device=latent.device)\n",
    "    if dtype == 'float16':\n",
    "        all_noise = all_noise.half()\n",
    "        scheduler.alphas_cumprod = scheduler.alphas_cumprod.half()\n",
    "\n",
    "    data = dict()\n",
    "    t_evaluated = set()\n",
    "    remaining_prmpt_idxs = list(range(len(text_embeds)))\n",
    "    start = T // max_n_samples // 2\n",
    "    t_to_eval = list(range(start, T, T // max_n_samples))[:max_n_samples]\n",
    "\n",
    "    for n_samples_val, n_to_keep_val in zip(n_samples, to_keep):\n",
    "        ts = []\n",
    "        noise_idxs = []\n",
    "        text_embed_idxs = []\n",
    "        curr_t_to_eval = t_to_eval[len(t_to_eval) // n_samples_val // 2::len(t_to_eval) // n_samples_val][:n_samples_val]\n",
    "        curr_t_to_eval = [t for t in curr_t_to_eval if t not in t_evaluated]\n",
    "        for prompt_i in remaining_prmpt_idxs:\n",
    "            for t_idx, t in enumerate(curr_t_to_eval, start=len(t_evaluated)):\n",
    "                ts.extend([t] * n_trials)\n",
    "                noise_idxs.extend(list(range(n_trials * t_idx, n_trials * (t_idx + 1))))\n",
    "                text_embed_idxs.extend([prompt_i] * n_trials)\n",
    "        t_evaluated.update(curr_t_to_eval)\n",
    "        pred_errors = eval_error(unet, scheduler, latent, all_noise, ts, noise_idxs,\n",
    "                                 text_embeds, text_embed_idxs, batch_size, dtype, loss)\n",
    "        # match up computed errors to the data\n",
    "        for prompt_i in remaining_prmpt_idxs:\n",
    "            mask = torch.tensor(text_embed_idxs) == prompt_i\n",
    "            prompt_ts = torch.tensor(ts)[mask]\n",
    "            prompt_pred_errors = pred_errors[mask]\n",
    "            if prompt_i not in data:\n",
    "                data[prompt_i] = dict(t=prompt_ts, pred_errors=prompt_pred_errors)\n",
    "            else:\n",
    "                data[prompt_i]['t'] = torch.cat([data[prompt_i]['t'], prompt_ts])\n",
    "                data[prompt_i]['pred_errors'] = torch.cat([data[prompt_i]['pred_errors'], prompt_pred_errors])\n",
    "\n",
    "        # compute the next remaining idxs\n",
    "        errors = [-data[prompt_i]['pred_errors'].mean() for prompt_i in remaining_prmpt_idxs]\n",
    "        best_idxs = torch.topk(torch.tensor(errors), k=n_to_keep_val, dim=0).indices.tolist()\n",
    "        remaining_prmpt_idxs = [remaining_prmpt_idxs[i] for i in best_idxs]\n",
    "\n",
    "    # organize the output\n",
    "    assert len(remaining_prmpt_idxs) == 1\n",
    "    pred_idx = remaining_prmpt_idxs[0]\n",
    "\n",
    "    return pred_idx, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def get_transform(interpolation=InterpolationMode.BICUBIC, size=512):\n",
    "    transform = torch_transforms.Compose([\n",
    "        torch_transforms.Resize(size, interpolation=interpolation),\n",
    "        torch_transforms.CenterCrop(size),\n",
    "        _convert_image_to_rgb,\n",
    "        torch_transforms.ToTensor(),\n",
    "        torch_transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "INTERPOLATIONS = {\n",
    "    'bilinear': InterpolationMode.BILINEAR,\n",
    "    'bicubic': InterpolationMode.BICUBIC,\n",
    "    'lanczos': InterpolationMode.LANCZOS,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run folder: ./Logs/cifar10/v2-0_1trials_5_1keep_50_500samples_l1\n",
      "Files already downloaded and verified\n",
      "Current path /home/zow/Gene-DRAIL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05ca483e40d4c668ff1585f69efe5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/834 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc: 87.86%: 100%|██████████| 834/834 [23:09:22<00:00, 99.95s/it]    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from diffusion.datasets import get_target_dataset\n",
    "import gc\n",
    "LOG_DIR = './Logs'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(dataset='cifar10', split='test', version='2-0', img_size=512, batch_size=32, n_trials=1,\n",
    "                   prompt_path='prompts/cifar10_prompts.csv', noise_path=None, subset_path=None, dtype='float16',\n",
    "                   interpolation='bicubic', extra=None, n_workers=12, worker_idx=0, load_stats=False, loss='l2',\n",
    "                   to_keep=[5, 1], n_samples=[50, 500]):\n",
    "    \n",
    "    assert len(to_keep) == len(n_samples)\n",
    "\n",
    "    # Make run output folder\n",
    "    name = f\"v{version}_{n_trials}trials_\"\n",
    "    name += '_'.join(map(str, to_keep)) + 'keep_'\n",
    "    name += '_'.join(map(str, n_samples)) + 'samples'\n",
    "    if interpolation != 'bicubic':\n",
    "        name += f'_{interpolation}'\n",
    "    if loss == 'l1':\n",
    "        name += '_l1'\n",
    "    elif loss == 'huber':\n",
    "        name += '_huber'\n",
    "    if img_size != 512:\n",
    "        name += f'_{img_size}'\n",
    "    if extra is not None:\n",
    "        run_folder = osp.join(LOG_DIR, dataset + '_' + extra, name)\n",
    "    else:\n",
    "        run_folder = osp.join(LOG_DIR, dataset, name)\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    print(f'Run folder: {run_folder}')\n",
    "\n",
    "    # Set up dataset and prompts\n",
    "    interpolation_func = INTERPOLATIONS[interpolation] #defined above\n",
    "    transform = get_transform(interpolation_func, img_size)\n",
    "    latent_size = img_size // 8\n",
    "    target_dataset = get_target_dataset(dataset, train=split == 'train', transform=transform)\n",
    "    print(f'Current path {os.getcwd()}')\n",
    "    # Corrected line: os.getcwd() instead of os.getcwd\n",
    "    prompts_df = pd.read_csv(osp.join(os.getcwd(), prompt_path))\n",
    "\n",
    "    # Load pretrained models\n",
    "    vae, tokenizer, text_encoder, unet, scheduler = get_sd_model(version, dtype)\n",
    "    vae = vae.to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    unet = unet.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Load noise\n",
    "    if noise_path is not None:\n",
    "        all_noise = torch.load(noise_path).to(device)\n",
    "        print('Loaded noise from', noise_path)\n",
    "    else:\n",
    "        all_noise = None\n",
    "\n",
    "    # Prepare text embeddings\n",
    "    text_input = tokenizer(prompts_df.prompt.tolist(), padding=\"max_length\",\n",
    "                           max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    embeddings = []\n",
    "    with torch.inference_mode():\n",
    "        for i in range(0, len(text_input.input_ids), 100):\n",
    "            text_embeddings = text_encoder(\n",
    "                text_input.input_ids[i: i + 100].to(device),\n",
    "            )[0]\n",
    "            embeddings.append(text_embeddings)\n",
    "    text_embeddings = torch.cat(embeddings, dim=0)\n",
    "    assert len(text_embeddings) == len(prompts_df)\n",
    "\n",
    "    # Subset of dataset to evaluate\n",
    "    if subset_path is not None:\n",
    "        idxs = np.load(subset_path).tolist()\n",
    "    else:\n",
    "        idxs = list(range(len(target_dataset)))\n",
    "    idxs_to_eval = idxs[worker_idx::n_workers]\n",
    "\n",
    "    formatstr = get_formatstr(len(target_dataset) - 1)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm.tqdm(idxs_to_eval)\n",
    "    for i in pbar:\n",
    "        if total > 0:\n",
    "            pbar.set_description(f'Acc: {100 * correct / total:.2f}%')\n",
    "        fname = osp.join(run_folder, formatstr.format(i) + '.pt')\n",
    "        if os.path.exists(fname):\n",
    "            print('Skipping', i)\n",
    "            if load_stats:\n",
    "                data = torch.load(fname)\n",
    "                correct += int(data['pred'] == data['label'])\n",
    "                total += 1\n",
    "            continue\n",
    "        image, label = target_dataset[i]\n",
    "        with torch.no_grad():\n",
    "            img_input = image.to(device).unsqueeze(0)\n",
    "            if dtype == 'float16':\n",
    "                img_input = img_input.half()\n",
    "            x0 = vae.encode(img_input).latent_dist.mean\n",
    "            x0 *= 0.18215\n",
    "        pred_idx, pred_errors = eval_prob_adaptive(unet, x0, text_embeddings, scheduler, n_samples, to_keep, batch_size, dtype, loss, latent_size, all_noise, n_trials)\n",
    "        pred = prompts_df.classidx[pred_idx]\n",
    "        torch.save(dict(errors=pred_errors, pred=pred, label=label), fname)\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage in a Jupyter notebook\n",
    "evaluate_model(\n",
    "    dataset='cifar10', \n",
    "    split='test', \n",
    "    n_trials=1, \n",
    "    to_keep=[5, 1], \n",
    "    n_samples=[50, 500], \n",
    "    loss='l1', \n",
    "    prompt_path='promps/cifar10_prompts.csv',\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   2492 MiB |   7272 MiB |  26593 MiB |  24101 MiB |\n",
      "|       from large pool |   2425 MiB |   7205 MiB |  26384 MiB |  23958 MiB |\n",
      "|       from small pool |     66 MiB |     67 MiB |    209 MiB |    143 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   2492 MiB |   7272 MiB |  26593 MiB |  24101 MiB |\n",
      "|       from large pool |   2425 MiB |   7205 MiB |  26384 MiB |  23958 MiB |\n",
      "|       from small pool |     66 MiB |     67 MiB |    209 MiB |    143 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   2473 MiB |   7253 MiB |  26479 MiB |  24006 MiB |\n",
      "|       from large pool |   2406 MiB |   7186 MiB |  26270 MiB |  23863 MiB |\n",
      "|       from small pool |     66 MiB |     67 MiB |    209 MiB |    142 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   2548 MiB |   7360 MiB |  22994 MiB |  20446 MiB |\n",
      "|       from large pool |   2476 MiB |   7288 MiB |  22922 MiB |  20446 MiB |\n",
      "|       from small pool |     72 MiB |     72 MiB |     72 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  57306 KiB | 241433 KiB |   4158 MiB |   4102 MiB |\n",
      "|       from large pool |  51408 KiB | 235471 KiB |   3704 MiB |   3654 MiB |\n",
      "|       from small pool |   5898 KiB |   9571 KiB |    453 MiB |    448 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1312    |    1320    |    2620    |    1308    |\n",
      "|       from large pool |     360    |     368    |     779    |     419    |\n",
      "|       from small pool |     952    |     956    |    1841    |     889    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1312    |    1320    |    2620    |    1308    |\n",
      "|       from large pool |     360    |     368    |     779    |     419    |\n",
      "|       from small pool |     952    |     956    |    1841    |     889    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     135    |     141    |     155    |      20    |\n",
      "|       from large pool |      99    |     105    |     119    |      20    |\n",
      "|       from small pool |      36    |      36    |      36    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      50    |      56    |     783    |     733    |\n",
      "|       from large pool |      24    |      35    |     425    |     401    |\n",
      "|       from small pool |      26    |      28    |     358    |     332    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genedrail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
